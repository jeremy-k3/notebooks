{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Implementation of RAG, using:\n",
        "* Langchain\n",
        "* FAISS\n",
        "* Sentence-transformers\n",
        "* an LLM: we will provide examples with Flan-alpaca-large, Zephyr-7b-beta and DeciLM-7b\n",
        "\n"
      ],
      "metadata": {
        "id": "J1hL9Ra9C975"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install required dependencies\n",
        "Langchain library is evolving very fast. If you use another version, some pieces of code may need to be changed."
      ],
      "metadata": {
        "id": "kO4A4ALxYWgQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQUOitxUC7Z4"
      },
      "outputs": [],
      "source": [
        "#Install all the dependencies\n",
        "!pip install  langchain~=0.0.352\n",
        "!pip install  pypdf\n",
        "!pip install  sentence-transformers==2.2.2\n",
        "!pip install  huggingface_hub\n",
        "!pip install  accelerate\n",
        "!pip install  torch~=2.1.2\n",
        "!pip install  transformers~=4.36.2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install FAISS\n",
        "With GPU support, install faiss-gpu\\\n",
        "With CPU only, install faiss-cpu\\\n",
        "Uncomment the line below that fits your hardware and run"
      ],
      "metadata": {
        "id": "SwMSPDSsZLMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu\n",
        "#!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "7a3UUKpyZYCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import modules"
      ],
      "metadata": {
        "id": "Bqlv8e3uZsAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Langchain modules\n",
        "from langchain import document_loaders as dl\n",
        "from langchain import embeddings\n",
        "from langchain import text_splitter as ts\n",
        "from langchain import vectorstores as vs\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.schema import StrOutputParser\n",
        "from langchain.schema.runnable import RunnablePassthrough\n",
        "from langchain.schema.runnable import RunnableParallel\n",
        "from langchain.prompts import PromptTemplate\n",
        "from operator import itemgetter\n",
        "#Torch + transformers\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "#Other useful modules\n",
        "import re\n",
        "import time\n"
      ],
      "metadata": {
        "id": "ipZPErNRDa2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the document and chunk it\n",
        "Upload your document beforehand.\\\n",
        "Use langchain text_splitter to make chunks. Each chunk as a page-content attribute and a metadata attribute which includes the document name and the page."
      ],
      "metadata": {
        "id": "aCDPD_ZlDcI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "document_path =\"quantum-mckinsey.pdf\"\n",
        "\n",
        "\n",
        "def split_doc(document_path, chunk_size=500, chunk_overlap=20):\n",
        "    loader = dl.PyPDFLoader(document_path)\n",
        "    document = loader.load()\n",
        "    text_splitter = ts.RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    document_splitted = text_splitter.split_documents(documents=document)\n",
        "    return document_splitted\n",
        "\n",
        "#Split the document and print the different chunks\n",
        "document_splitted = split_doc(document_path)\n",
        "for doc in document_splitted:\n",
        "  print(doc)"
      ],
      "metadata": {
        "id": "peR77UC4Ds9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the embedding model\n",
        "Firstly, we will store the sentence-transformers model locally and then load it.\\\n",
        "If you are not running on GPU, change the device to cpu."
      ],
      "metadata": {
        "id": "b5ziQ6sUEI9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
        "#Save the model locally\n",
        "model.save('sentence-transformers')\n",
        "del model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "V1Z4Bjxvbgos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embedding_model():\n",
        "    model_kwargs = {'device': 'cuda:0'}\n",
        "    encode_kwargs = {'normalize_embeddings': False}\n",
        "    embedding_model_instance = embeddings.HuggingFaceEmbeddings(\n",
        "        model_name=\"sentence-transformers\",\n",
        "        model_kwargs=model_kwargs,\n",
        "        encode_kwargs=encode_kwargs\n",
        "    )\n",
        "    return embedding_model_instance\n",
        "\n",
        "#Instantiate the embedding model\n",
        "embedding_model_instance = load_embedding_model()"
      ],
      "metadata": {
        "id": "BQObNKu6EMBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a vector database to store embeddings\n",
        "We will use FAISS to store the chunks and their embeddings. We also store the metadata.\n",
        "\n"
      ],
      "metadata": {
        "id": "IaLAwIKQERyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_db(document_splitted, embedding_model_instance):\n",
        "\n",
        "    model_vectorstore = vs.FAISS\n",
        "    db=None\n",
        "    try:\n",
        "        content = []\n",
        "        metadata = []\n",
        "        for d in document_splitted:\n",
        "            content.append(d.page_content)\n",
        "            metadata.append({'source': d.metadata})\n",
        "        db=model_vectorstore.from_texts(content, embedding_model_instance, metadata)\n",
        "    except Exception as error:\n",
        "        print(error)\n",
        "    return db\n",
        "\n",
        "db = create_db(document_splitted, embedding_model_instance)\n",
        "#store the db locally for future use\n",
        "db.save_local('db.index')"
      ],
      "metadata": {
        "id": "ByFlE0tTEUPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the large language model\n",
        "We will provide here different options of LLMs. If you are running this notebook on Google Colab and do not have enough RAM or GPU memory, use the flan-alpaca-large model. Otherwise, choose between Zephyr-7b-beta and DeciLM-7b.\n",
        "You can of course use any other LLM available on Huggingface.\\\n",
        "**In any case, only use one of the 3 following models**"
      ],
      "metadata": {
        "id": "glzwaoBSG30D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flan-alpaca-large\n",
        "https://huggingface.co/declare-lab/flan-alpaca-large"
      ],
      "metadata": {
        "id": "8P2RkIO5vMzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model locally.\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"declare-lab/flan-alpaca-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"declare-lab/flan-alpaca-large\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
        "model.save_pretrained('flan-alpaca-large-model', max_shard_size=\"1000MB\")\n",
        "tokenizer.save_pretrained('flan-alpaca-large-tokenizer')\n",
        "del model\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "gMmdHejEve3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a pipeline with the local version of the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"flan-alpaca-large-tokenizer\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"flan-alpaca-large-model\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
        "pipe = pipeline(task=\"text2text-generation\", model=model,tokenizer=tokenizer, device=\"cuda:0\", max_new_tokens=1000)"
      ],
      "metadata": {
        "id": "-qPC17kZvqMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zephyr-7b-beta\n",
        "https://huggingface.co/HuggingFaceH4/zephyr-7b-beta"
      ],
      "metadata": {
        "id": "7fWpfPUTvxmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model locally.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
        "model.save_pretrained('zephyr-7b-beta-model', max_shard_size=\"1000MB\")\n",
        "tokenizer.save_pretrained('zephyr-7b-beta-tokenizer')\n",
        "del model\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "bWV7rk2udxYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a pipeline with the local version of the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"zephyr-7b-beta-tokenizer\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"zephyr-7b-beta-model\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
        "pipe = pipeline(task=\"text-generation\", model=model,tokenizer=tokenizer, device=\"cuda:0\", max_new_tokens=1000)"
      ],
      "metadata": {
        "id": "jocfq2lrG6Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DeciLM-7b\n",
        "https://huggingface.co/Deci/DeciLM-7B"
      ],
      "metadata": {
        "id": "mfE9dvIGwL0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Save the model locally.\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Deci/DeciLM-7B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Deci/DeciLM-7B\", torch_dtype=torch.float16, trust_remote_code=True).to(device)\n",
        "model.save_pretrained('DeciLM-7b-model', max_shard_size=\"1000MB\")\n",
        "tokenizer.save_pretrained('DeciLM-7b-tokenizer')\n",
        "del model\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "cnamCcWywW19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a pipeline with the local version of the model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeciLM-7b-tokenizer\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"DeciLM-7b-model\", low_cpu_mem_usage=True, torch_dtype=torch.float16, trust_remote_code=True)\n",
        "pipe = pipeline(task=\"text-generation\", model=model,tokenizer=tokenizer, device=\"cuda:0\", max_new_tokens=1000, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "zVULZ0Mcw1H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect the pipeline with Langchain"
      ],
      "metadata": {
        "id": "T4s-upgxwFvd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use the pipeline in Langchain\n",
        "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0})"
      ],
      "metadata": {
        "id": "z2qS5EBkv_Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load a retriever, define prompt template and chains"
      ],
      "metadata": {
        "id": "lf5yM4lTHSvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is quantum computing?\"\n",
        "retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"k\": 6, 'score_threshold': 0.01})\n",
        "retrieved_docs = retriever.get_relevant_documents(query)"
      ],
      "metadata": {
        "id": "6KJEBftsHUSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
        "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "rag_prompt_custom = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "f1UrhKAzBPa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "#First chain to query the LLM\n",
        "rag_chain_from_docs = (\n",
        "    {\n",
        "        \"context\": lambda input: format_docs(input[\"documents\"]),\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt_custom\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "#Second chain to postprocess the answer\n",
        "rag_chain_with_source = RunnableParallel(\n",
        "    {\"documents\": retriever, \"question\": RunnablePassthrough()}\n",
        ") | {\n",
        "    \"documents\": lambda input: [doc.metadata for doc in input[\"documents\"]],\n",
        "    \"answer\": rag_chain_from_docs,\n",
        "}"
      ],
      "metadata": {
        "id": "Ghp5eDbvBSmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query the LLM and postprocess the answer"
      ],
      "metadata": {
        "id": "cTGc4vtbHmDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t0=time.time()\n",
        "resp = rag_chain_with_source.invoke(query)\n",
        "if len(resp['documents'])==0:\n",
        "  print('No documents found')\n",
        "else:\n",
        "  stripped_resp = re.sub(r\"\\n+$\", \" \", resp['answer'])\n",
        "  print(stripped_resp)\n",
        "  print('Sources',resp['documents'])\n",
        "  print('Response time:', time.time()-t0)\n"
      ],
      "metadata": {
        "id": "i_v8v20pHpDo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}